{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGR: [[0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " ...\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "SH_path = 'C:\\Workspace\\Personal\\ccp_data\\Training\\image\\AP10_City_SH/AP_10_SH_CT_37705043_0770.tif'\n",
    "\n",
    "SH = cv2.imread(SH_path)\n",
    "SH20 = SH*20\n",
    "\n",
    "cv2.imshow('Image', SH20)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "unique_pixels = np.unique(SH, axis=0)\n",
    "\n",
    "# 각 유니크한 픽셀 값 출력\n",
    "for pixel in unique_pixels:\n",
    "    print(\"BGR:\", pixel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LBW\\anaconda3\\envs\\torch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After self-attention: torch.Size([10, 20, 512])\n",
      "After feedforward: torch.Size([10, 20, 512])\n",
      "After layer 1: torch.Size([10, 20, 512])\n",
      "After self-attention: torch.Size([10, 20, 512])\n",
      "After feedforward: torch.Size([10, 20, 512])\n",
      "After layer 2: torch.Size([10, 20, 512])\n",
      "After self-attention: torch.Size([10, 20, 512])\n",
      "After feedforward: torch.Size([10, 20, 512])\n",
      "After layer 3: torch.Size([10, 20, 512])\n",
      "After self-attention: torch.Size([10, 20, 512])\n",
      "After feedforward: torch.Size([10, 20, 512])\n",
      "After layer 4: torch.Size([10, 20, 512])\n",
      "After self-attention: torch.Size([10, 20, 512])\n",
      "After feedforward: torch.Size([10, 20, 512])\n",
      "After layer 5: torch.Size([10, 20, 512])\n",
      "After self-attention: torch.Size([10, 20, 512])\n",
      "After feedforward: torch.Size([10, 20, 512])\n",
      "After layer 6: torch.Size([10, 20, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SegFormerDecoder(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, num_layers):\n",
    "        super(SegFormerDecoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            SegFormerDecoderLayer(input_dim, num_heads)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            print(f\"After layer {i + 1}: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "class SegFormerDecoderLayer(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads):\n",
    "        super(SegFormerDecoderLayer, self).__init__()\n",
    "        self.self_attention = nn.MultiheadAttention(input_dim, num_heads)\n",
    "        self.norm1 = nn.LayerNorm(input_dim)\n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(input_dim, 4 * input_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * input_dim, input_dim)\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Self-Attention\n",
    "        attn_output, _ = self.self_attention(x, x, x)\n",
    "        x = x + attn_output\n",
    "        x = self.norm1(x)\n",
    "        print(f\"After self-attention: {x.shape}\")\n",
    "\n",
    "        # Feedforward\n",
    "        ff_output = self.feedforward(x)\n",
    "        x = x + ff_output\n",
    "        x = self.norm2(x)\n",
    "        print(f\"After feedforward: {x.shape}\")\n",
    "\n",
    "        return x\n",
    "\n",
    "# 예제 사용\n",
    "input_dim = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "\n",
    "decoder = SegFormerDecoder(input_dim, num_heads, num_layers)\n",
    "\n",
    "# 입력 데이터\n",
    "input_data = torch.randn(10, 20, input_dim)  # (sequence_length, batch_size, input_dim)\n",
    "\n",
    "# 디코더에 전달\n",
    "output_data = decoder(input_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "from math import sqrt\n",
    "\n",
    "class DsConv2d(nn.Module):\n",
    "    '''\n",
    "    Mix-FFN에 Positional encoding을 대신할 3x3 Depthwise separable convolution\n",
    "    '''\n",
    "    def __init__(self, dim_in, dim_out, kernel_size, padding, stride=1, bias=True):\n",
    "        super(DsConv2d, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # Depthwise Convolution\n",
    "            nn.Conv2d(in_channels=dim_in,\n",
    "                      out_channels=dim_in,\n",
    "                      kernel_size=kernel_size,\n",
    "                      stride=stride,\n",
    "                      padding=padding,\n",
    "                      groups=dim_in,\n",
    "                      bias=bias\n",
    "                      ),\n",
    "            # Pointwise Convolution\n",
    "            nn.Conv2d(in_channels=dim_in,\n",
    "                      out_channels=dim_out,\n",
    "                      kernel_size=1,\n",
    "                      bias=bias\n",
    "                      )\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "class LayerNorm(nn.Module):\n",
    "    '''\n",
    "    Efficient Self-Attn block과 Mix-FFN block 전에 사용될 Layer Normalization\n",
    "    '''\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.g = nn.Parameter(torch.ones(1, dim, 1, 1))\n",
    "        self.b = nn.Parameter(torch.zeros(1, dim, 1, 1))\n",
    "    def forward(self, x):\n",
    "        std = torch.var(x, dim=1, unbiased=False, keepdim=True).sqrt()  # std = root(variation)\n",
    "        mean = torch.mean(x, dim=1, keepdim=True)\n",
    "        return (x - mean) / (std + self.eps) * self.g + self.b\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super(PreNorm, self).__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = LayerNorm(dim)\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x))\n",
    "class EfficientSelfAttention(nn.Module):\n",
    "    def __init__(self, *, dim, heads, reduction_ratio):\n",
    "        super(EfficientSelfAttention, self).__init__()\n",
    "        self.scale = (dim // heads) ** -0.5  # root(dim head)\n",
    "        self.heads = heads\n",
    "        self.to_q = nn.Conv2d(dim, dim, 1, bias=False)\n",
    "        self.to_kv = nn.Conv2d(dim, dim*2, reduction_ratio, stride=reduction_ratio, bias=False)  # ESA\n",
    "        self.to_out = nn.Conv2d(dim, dim, 1, bias=False)\n",
    "    def forward(self, x):\n",
    "        h, w = x.shape[-2:]\n",
    "        heads = self.heads\n",
    "        q, k, v = (self.to_q(x), *self.to_kv(x).chunk(2, dim=1))  # chunk: dim=1을 기준으로 2개로 쪼갬\n",
    "        # q,k,v의 차원을 모두 아래와 같이 변경\n",
    "        # batch, (head*channel), w, h -> (batch*head), (w*h), channel\n",
    "        # 첫 stage에서 16384 -> 256 으로 행렬곱 연산을 64배 감소시킴(Efficient multi head self attention)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b (h c) x y -> (b h) (x y) c', h=heads), (q, k, v))\n",
    "        # einsum을 아래와 같이 행렬곱으로 사용 가능, q@k로도 가능\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale  # Q*K / root(dim head)\n",
    "        attn = sim.softmax(dim=-1)  # dim=-1 -> channel\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)  # 최종 attention 계산\n",
    "        out = rearrange(out, '(b h) (x y) c -> b (h c) x y', h=heads, x=h, y=w)  # 차원 복구\n",
    "        return self.to_out(out)\n",
    "class MixFeedForward(nn.Module):\n",
    "    def __init__(self, *, dim, expansion_factor):\n",
    "        super(MixFeedForward, self).__init__()\n",
    "        hidden_dim = dim * expansion_factor\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(dim, hidden_dim, 1),\n",
    "            DsConv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1),  # positional embedding 대체\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(hidden_dim, dim, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "class MiT(nn.Module):\n",
    "    '''\n",
    "    Mix Transformer encoders\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 channels,\n",
    "                 dims,\n",
    "                 heads,\n",
    "                 ff_expansion,\n",
    "                 reduction_ratio,\n",
    "                 num_layers\n",
    "                 ):\n",
    "        super(MiT, self).__init__()\n",
    "        stage_kernel_stride_pad = ((7, 4, 3), (3, 2, 1), (3, 2, 1), (3, 2, 1))\n",
    "        dims = (channels, *dims)  # *: take off tuple  (3, 32, 64, 160, 256)\n",
    "        dim_pairs = list(zip(dims[:-1], dims[1:]))  # [(3, 32), (32, 64), (64, 160), (160, 256)]\n",
    "        self.stages = nn.ModuleList([])\n",
    "        for (dim_in, dim_out), (kernel, stride, padding), num_layers, ff_expansion, heads, reduction_ratio in zip(dim_pairs, stage_kernel_stride_pad, num_layers, ff_expansion, heads, reduction_ratio):\n",
    "            get_overlap_patches = nn.Unfold(kernel, stride=stride, padding=padding)\n",
    "            overlap_patch_embed = nn.Conv2d(dim_in * kernel ** 2, dim_out, 1)\n",
    "            layers = nn.ModuleList([])\n",
    "            # Layer Norm -> ESA -> Layer Norm -> MFFN\n",
    "            for _ in range(num_layers):\n",
    "                layers.append(nn.ModuleList([\n",
    "                    PreNorm(dim_out, EfficientSelfAttention(dim=dim_out, heads=heads, reduction_ratio=reduction_ratio)),\n",
    "                    PreNorm(dim_out, MixFeedForward(dim=dim_out, expansion_factor=ff_expansion)),\n",
    "                ]))\n",
    "            self.stages.append(nn.ModuleList([\n",
    "               get_overlap_patches,\n",
    "               overlap_patch_embed,\n",
    "               layers\n",
    "            ]))\n",
    "    def forward(self, x, return_layer_outputs=False):\n",
    "        h, w = x.shape[-2:]\n",
    "        layer_outputs = []\n",
    "        for (get_overlap_patches, overlap_embed, layers) in self.stages:\n",
    "            x = get_overlap_patches(x)  # (b, c x kernel x kernel, num_patches)\n",
    "            num_patches = x.shape[-1]\n",
    "            ratio = int(sqrt(h * w / num_patches))\n",
    "            x = rearrange(x, 'b c (h w) -> b c h w', h=h // ratio)  # (b, c(cxkernelxkernel), h, w)\n",
    "            x = overlap_embed(x)  # (b c(embed dim) h w)\n",
    "            for (attn, ff) in layers:  # attention, feed forward\n",
    "                x = attn(x) + x  # skip connection\n",
    "                x = ff(x) + x\n",
    "            layer_outputs.append(x)  # multi scale features\n",
    "        return x if not return_layer_outputs else layer_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT Shape: torch.Size([2, 4, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "class SegFormer(nn.Module):\n",
    "    '''\n",
    "    Default values from Mix Transformer B0\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 dims=(32, 64, 160, 256),\n",
    "                 heads=(1, 2, 5, 8),\n",
    "                 ff_expansion=(8, 8, 4, 4),\n",
    "                 reduction_ratio=(8, 4, 2, 1),\n",
    "                 num_layers=(2, 2, 2, 2),\n",
    "                 channels=3,\n",
    "                 decoder_dim=256,\n",
    "                 num_classes=4):\n",
    "        super(SegFormer, self).__init__()\n",
    "        assert all([*map(lambda t: len(t) == 4, (dims, heads, ff_expansion, reduction_ratio, num_layers))]), 'only four stages are allowed, all keyword arguments must be either a single value or a tuple of 4 values'\n",
    "        self.mit = MiT(\n",
    "            channels=channels,\n",
    "            dims=dims,\n",
    "            heads=heads,\n",
    "            ff_expansion=ff_expansion,\n",
    "            reduction_ratio=reduction_ratio,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        # 논문 모델 그림에서 MLP Layer\n",
    "        self.to_fused = nn.ModuleList([nn.Sequential(\n",
    "            nn.Conv2d(dim, decoder_dim, 1),  # First, multi-level features Fi from the MiT encoder go through an MLP layer to unify the channel dimension.\n",
    "            nn.Upsample(scale_factor=2**i)  # second step, features are up-sampled to 1/4th and concatenated together.\n",
    "        ) for i, dim in enumerate(dims)])\n",
    "        self.to_segmentation = nn.Sequential(\n",
    "            nn.Conv2d(4 * decoder_dim, decoder_dim, 1),  # Third, a MLP layer is adopted to fuse the concatenated features\n",
    "            nn.Conv2d(decoder_dim, num_classes, 1),  # Finally, another MLP layer takes the fused feature to predict the segmentation mask M with a H4 × W4 × Ncls resolution\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        layer_outputs = self.mit(x, return_layer_outputs=True)\n",
    "        fused = [to_fused(output) for output, to_fused in zip(layer_outputs, self.to_fused)]\n",
    "        fused = torch.cat(fused, dim=1)\n",
    "        return self.to_segmentation(fused)\n",
    "if __name__ == '__main__':\n",
    "    model = SegFormer()\n",
    "    img = torch.rand((2, 3, 512, 512))\n",
    "    print(f'OUTPUT Shape: {model(img).shape}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
